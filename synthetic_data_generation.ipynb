{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>2708</td><td>application_1595489208176_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://lx69384.sbcore.net:8088/proxy/application_1595489208176_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://lx69383.sbcore.net:8042/node/containerlogs/container_e33_1595489208176_0010_01_000001/CashFlow_Algirdas__philipp1\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# Packages import\n",
    "# ===============\n",
    "\n",
    "from __future__ import division\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import yaml\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy import signal\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.types as pst\n",
    "from pyspark.sql.functions import udf\n",
    "from utils import *\n",
    "import pydoop.hdfs as pydoop\n",
    "from hops import hdfs\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"false\")\n",
    "\n",
    "cwd = os.getcwd() + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Reading configuration files\n",
    "# ===========================\n",
    "\n",
    "data_conf = Get_Data_From_JSON(cwd + \"data.json\")\n",
    "model_conf = Get_Data_From_JSON(cwd + \"config.json\")\n",
    "\n",
    "start_date, end_date = data_conf['start_date'], data_conf['end_date']\n",
    "N_days_X, N_days_y = int(data_conf['number_of_historical_days']), int(data_conf['number_of_predicted_days'])  # 365, 92\n",
    "\n",
    "end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "start_date_for_prediction_dt = end_date_dt - relativedelta(days=N_days_X + N_days_y)\n",
    "start_date_for_prediction = start_date_for_prediction_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "start_date_dt, end_date_dt, start_date_prediction, end_date_prediction, end_date_plusOneDay, end_date_minus_6month = dates_definitions(\n",
    "    start_date, end_date, N_days_X, N_days_y)\n",
    "\n",
    "time_range = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "# Type of dataset desired\n",
    "# Case we want a dataset to train a model: use 1e5 and serving_mode=False\n",
    "# Case we want an unseen dataset to serve the model on: use 2.5e6 and serving_mode=True\n",
    "N_customers = 1e5#2.5e6\n",
    "serving_mode = False  # True if creating data for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========\n",
    "# Functions\n",
    "# =========\n",
    "\n",
    "def time_series_generator(size=500,\n",
    "                          cycle_period=30.5,\n",
    "                          signal_type='sine',\n",
    "                          salary=1,\n",
    "                          trend=0.1,\n",
    "                          noise=0.1,\n",
    "                          offset=False,\n",
    "                          spike=0):\n",
    "    '''\n",
    "    This function generates mock time series with noise\n",
    "    :param (int) size: length of the time series\n",
    "    :param (float) cycle_period: period of the signal (usually 30.5, the month period, in days)\n",
    "    :param (string) signal_type: Type of signal, \"sine\", \"sawtooth\", \"triangle\", \"square\", \"random_choice\"\n",
    "    :param (float) salary: Base scaling variable for the trend, default=1\n",
    "    :param (float) trend: Scaling variable for the trend\n",
    "    :param (float) noise: Trend noise, default=0.1\n",
    "    :param (boolean) offset: Use of random phase offset, makes seasonality\n",
    "    :param (int) spike: Number of random amplitude spikes\n",
    "    :return (numpy array): Timeseries with account balance for each day\n",
    "    '''\n",
    "\n",
    "    signal_types = ['sine', 'sawtooth', 'triangle', 'square']\n",
    "    if signal_type == 'random_choice':\n",
    "        signal_type = random.choice(signal_types)\n",
    "    elif signal_type not in signal_types:\n",
    "        raise ValueError('{} is not a valid signal type'.format(signal_type))\n",
    "\n",
    "    # in size = 635, and cycle_period = 30.5, we have ~ 21 periods (20.8)\n",
    "    count_periods = size / cycle_period\n",
    "\n",
    "    # 1. The trend making\n",
    "    t = np.linspace(-0.5 * cycle_period * count_periods, 0.5 * cycle_period * count_periods, size)\n",
    "    t_trend = np.linspace(0, 1, size)\n",
    "    sign = random.choice([-1, 1])\n",
    "    trend_ts = sign * salary * np.exp(trend*t_trend)\n",
    "\n",
    "    # 2. The seasonality making\n",
    "    if offset:\n",
    "        phase = np.random.uniform(-1, 1) * np.pi\n",
    "    else:\n",
    "        phase = 0\n",
    "\n",
    "    if signal_type == 'sine':     ts = 0.5 * salary * np.sin(2 * np.pi * (1. / cycle_period) * t + phase)\n",
    "    if signal_type == 'sawtooth': ts = -0.5 * salary * signal.sawtooth(2 * np.pi * (1. / cycle_period) * t + phase)\n",
    "    if signal_type == 'triangle': ts = 1 * salary * np.abs(signal.sawtooth(2 * np.pi * (1. / cycle_period) * t + phase)) - 1\n",
    "    if signal_type == 'square':   ts = 0.5 * salary * signal.square(2 * np.pi * (1. / cycle_period) * t + phase)\n",
    "\n",
    "    # 3. The noise making\n",
    "    noise_ts = np.random.normal(0, noise * salary, size)\n",
    "\n",
    "    ts = ts + trend_ts + noise_ts\n",
    "\n",
    "    # 4. Adding spikes to the time series\n",
    "    if spike > 0:\n",
    "        last_spike_time = int(size)-92      # Don't create spikes in the last 3 months, where we want to predict\n",
    "        first_spike_time = int(size)-92-365 # Let's have the spikes within 1 year up to the prediction time\n",
    "        for _ in range(spike):\n",
    "            sign = random.choice([-1, 1])\n",
    "            t_spike = np.random.randint(first_spike_time, last_spike_time)  # time of the spike\n",
    "            ts[t_spike:] = ts[t_spike:] + sign * np.random.normal(3 * salary, salary)\n",
    "            print(t_spike)\n",
    "            \n",
    "    print(size, first_spike_time, last_spike_time)\n",
    "            \n",
    "    if signal_type == 'sine':     signal_type_int = 1\n",
    "    if signal_type == 'triangle': signal_type_int = 2\n",
    "    if signal_type == 'square':   signal_type_int = 3     \n",
    "    if signal_type == 'sawtooth': signal_type_int = 4      \n",
    "\n",
    "    return np.around(ts,decimals=2).tolist(), signal_type_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+\n",
      "|primaryaccountholder|     transactiondate|             balance|signal_type|\n",
      "+--------------------+--------------------+--------------------+-----------+\n",
      "|                   0|[2018-12-01, 2018...|[23367.93, 23041....|          3|\n",
      "|                   1|[2018-12-01, 2018...|[9823.69, 8871.93...|          2|\n",
      "|                   2|[2018-12-01, 2018...|[9708.74, 8577.31...|          2|\n",
      "|                   3|[2018-12-01, 2018...|[-4881.65, -5710....|          1|\n",
      "|                   4|[2018-12-01, 2018...|[24137.48, 26038....|          2|\n",
      "+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "100000"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Generation of synthetic data (for both formats)\n",
    "# ============================\n",
    "\n",
    "dff = spark.range(N_customers).toDF(\"primaryaccountholder\") #'primaryaccountholder','transactiondate','balance'\n",
    "\n",
    "#@udf(\"array<float>\") \n",
    "def ts_generation():\n",
    "    bb,nn = time_series_generator(\n",
    "              size=len(time_range),\n",
    "              cycle_period=30.5,\n",
    "              signal_type='random_choice',\n",
    "              salary=np.maximum(np.random.normal(15000, 5000), 100),\n",
    "              trend=np.random.uniform(1,2),#np.random.normal(0, 1.1),\n",
    "              noise=np.abs(np.random.normal(0, 0.01)) + 0.1,\n",
    "              offset=True,\n",
    "              spike=3)      \n",
    "    return Row('signal_type', 'balance')(nn, bb)\n",
    "    \n",
    "schema = pst.StructType([\n",
    "    pst.StructField(\"signal_type\", pst.IntegerType(), False),\n",
    "    pst.StructField(\"balance\", pst.ArrayType(pst.FloatType()), False)])    \n",
    "    \n",
    "ts_generation_udf = F.udf(ts_generation, schema)  \n",
    "  \n",
    "dff = dff.withColumn(\"generation\", ts_generation_udf())\n",
    "\n",
    "dff = dff.select('primaryaccountholder', \"generation.*\")\n",
    "\n",
    "dff2 = spark.sql(\"SELECT sequence(to_date('{0}'), to_date('{1}'), interval 1 day) as transactiondate\".format(start_date, end_date))\n",
    "\n",
    "timeseries_spark = dff2.crossJoin(dff)\n",
    "timeseries_spark = timeseries_spark.select('primaryaccountholder','transactiondate','balance','signal_type')\n",
    "\n",
    "timeseries_spark.show(5)\n",
    "timeseries_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Saving the dataset\n",
    "# ========================\n",
    "\n",
    "if not serving_mode:\n",
    "    table_out = data_conf['synthetic_data']['table_to_train_on']\n",
    "else:    \n",
    "    table_out = data_conf['synthetic_data']['table_to_score']\n",
    "    \n",
    "#timeseries_spark.write.format(\"parquet\").mode(\"overwrite\").save(cwd+\"{0}.parquet\".format(table_out)) #ideally write like this\n",
    "timeseries_spark.write.format(\"parquet\").mode(\"overwrite\").save(\n",
    "        \"hdfs:///Projects/CashFlow_Algirdas/CashFlow_Algirdas_Training_Datasets/{0}.parquet\".format(table_out)) #so far still like this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  },
  "name": "mock_data_generation",
  "notebookId": 2786978204282827
 },
 "nbformat": 4,
 "nbformat_minor": 4
}